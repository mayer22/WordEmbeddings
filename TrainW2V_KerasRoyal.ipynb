{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f3eea2",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('royalData.txt', 'r')\n",
    "royal_data = file.readlines() \n",
    "print(royal_data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369aa51",
   "metadata": {},
   "source": [
    "# Clean and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402d071-72c1-4bdf-9cc8-98ad320e9f9c",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Comment out the noted line to include stop words the second time around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with all lowercase characters and remove sentence stops\n",
    "for i in range(len(royal_data)):\n",
    "    royal_data[i] = royal_data[i].lower().replace('\\n', '')\n",
    "\n",
    "# tokenize and remove stop words\n",
    "stopwords = ['the', 'is', 'will', 'be', 'a', 'only', 'can', 'their', 'now', 'and', 'at', 'it','in']\n",
    "filtered_data = []\n",
    "for sent in royal_data:\n",
    "    temp = []\n",
    "    for word in sent.split():\n",
    "        if word not in stopwords:  #COMMENT OUT THIS LINE TO INCLUDE STOP WORDS\n",
    "            temp.append(word)\n",
    "    filtered_data.append(temp)\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286015a-4f0b-4bc6-a450-44fd7063876d",
   "metadata": {},
   "source": [
    "### Create unique word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438f3b6-baac-43d4-8014-0514c2a4b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for fd in filtered_data:\n",
    "    all_words.extend(fd) #get all words in one list\n",
    "all_words = list(set(all_words)) #turn list into a set to get unique words and then back into a list\n",
    "all_words.sort()\n",
    "\n",
    "# turn unique word list into a dictionary\n",
    "words_dict = {}\n",
    "counter = 0\n",
    "for word in all_words:\n",
    "    words_dict[word] = counter\n",
    "    counter += 1\n",
    "\n",
    "print(words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c060ff9",
   "metadata": {},
   "source": [
    "# Creating our co-occurrence \"matrix\"\n",
    "Create a list of all possible word combinations in a sentence and their opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb52dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "for words_list in filtered_data:\n",
    "    for i in range(len(words_list) - 1):\n",
    "        for j in range(i+1, len(words_list)):\n",
    "            bigrams.append([words_list[i], words_list[j]])\n",
    "            bigrams.append([words_list[j], words_list[i]])\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba98e1-116f-4b01-a520-704569fffbe7",
   "metadata": {},
   "source": [
    "**DON'T RUN THIS NEXT CELL THE FIRST TIME THROUGH**\n",
    "\n",
    "Alternative method that create our co-occurence matrix with a time window = 1 (one word before and one word after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45a99b-26f8-4a7b-9bea-21cd5d1d9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "for words_list in filtered_data:\n",
    "    for i in range(len(words_list) - 1):\n",
    "        bigrams.append([words_list[i], words_list[i+1]])\n",
    "        bigrams.append([words_list[i+1], words_list[i]])\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f911c7d",
   "metadata": {},
   "source": [
    "# Performing one-hot encoding\n",
    "Create an identity matrix with the unique words on the rows and columns and a '1' in the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "onehot_data = np.zeros((len(all_words), len(all_words))) #initialize array\n",
    "for i in range(len(all_words)): #create identity matrix\n",
    "    onehot_data[i][i] = 1\n",
    "\n",
    "# map words to the vectors\n",
    "onehot_dict = {}\n",
    "counter = 0\n",
    "for word in all_words:\n",
    "    onehot_dict[word] = onehot_data[counter]\n",
    "    counter += 1\n",
    "\n",
    "for word in onehot_dict:\n",
    "    print(word, \":\", onehot_dict[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142aeb75-fb13-4821-94e1-bb160075f0fe",
   "metadata": {},
   "source": [
    "Match one-hot encoded input vectors and output labels for training the model\n",
    "\n",
    "Uses the co-occurrence matrix and the one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for bi in bigrams:\n",
    "    X.append(onehot_dict[bi[0]]) #first word in each bigram\n",
    "    Y.append(onehot_dict[bi[1]]) #second word in each bigram\n",
    "# turn into an array\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218441d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23993cc-d1e6-45ec-aa6a-c9fdb911b462",
   "metadata": {},
   "source": [
    "### Define model\n",
    "We will demonstrate our first model using a hidden layer of size 2\n",
    "\n",
    "This means that the resulting word vector embedding will be of length 2\n",
    "\n",
    "This will make visualization easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733cf33-158c-4d6a-bba0-e242cd90218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "kmodel = Sequential()\n",
    "vocab_size = len(onehot_data[0])\n",
    "embed_size = 2 # of parameters\n",
    "\n",
    "kmodel.add(Input(shape = (vocab_size,)))\n",
    "kmodel.add(Dense(embed_size))\n",
    "kmodel.add(Dense(vocab_size, activation='softmax')) #last layer must be softmax activation\n",
    "\n",
    "kmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam') #we use this loss because of softmax activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be049f-e80e-4bea-88de-6cece26e772e",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN ME IN LECTURE\n",
    "\n",
    "kmodel.fit(X, Y, epochs = 1000, batch_size = 256, verbose = False) #Pass X as the input and Y as the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485adf6-5000-48fe-b570-a300feebe438",
   "metadata": {},
   "source": [
    "### Extract weights from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = kmodel.get_weights()[0]\n",
    "\n",
    "word_embeddings = {}\n",
    "for word in all_words:\n",
    "    word_embeddings[word] = weights[words_dict[word]] #Match weights to words\n",
    "\n",
    "for word in all_words:\n",
    "    print(word, \":\", word_embeddings[word]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578e5dc-57d0-4fa3-9818-a81f4daae437",
   "metadata": {},
   "source": [
    "# **STOP HERE AND GO BACK TO SLIDES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404160f-b2aa-40a3-a4ed-1cdb0e6c287b",
   "metadata": {},
   "source": [
    "# Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for word in list(words_dict.keys()):\n",
    "    coord = word_embeddings.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))\n",
    "\n",
    "plt.savefig('img.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4efe-790d-4d02-9847-c870467754d7",
   "metadata": {},
   "source": [
    "### Rerun using +/- 1 word only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51918105-07be-4b86-a1ea-d10599cc8857",
   "metadata": {},
   "source": [
    "### Rerun with stop words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f06e3119828c145c4028f0fee76387e74b42caa79bccb80b4ba64fe213bb9e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
